sampling.rate = 0.8
kTestVals = c()
# Running K-fold cross validation for K values from 1-10
for(k in 1:10)
{
AllErrors = c()
for(fold in 1:5)
{
# creating Training and testing Sets
training <- sample(1:num_samples, sampling.rate*num_samples, replace=FALSE)
trainingSet <- subset(labeledData[training, ])
testing <- setdiff(1:num_samples, training)
testingSet <- subset(labeledData[testing,])
# Getting features of training set
trainingfeatures <- subset(trainingSet, select=c(-like))
# Get the labels of the training set
traininglabels <- trainingSet$like
#Get the features of the testing set
testingfeatures <- subset(testingSet, select=c(-like))
predictions = knn(trainingfeatures, testingfeatures, traininglabels, k = k)
errors = sum(predictions != testingSet$like)
mse = mean(errors^2)
AllErrors[fold] = mse
}
AverageError = mean(AllErrors)
kTestVals[k] = AverageError
}
# Retrieving K value with smallest average error after validation test
optimalK = which.min(kTestVals)
# partitioning training and testing sets
trainingSet <- labeledData
testingSet <- unlabeledData
# Getting features of training set
trainingfeatures <- subset(trainingSet, select=c(-like))
# Get the labels of the training set
traininglabels <- trainingSet$like
#Get the features of the testing set
testingfeatures <- subset(testingSet, select=c(-like))
predictions = knn(trainingfeatures, testingfeatures, traininglabels, k = optimalK)
print("Question 1. KNN results:")
print(predictions)
library(class)
predictedLabels = knn(trainingfeatures, testingfeatures, traininglabels, k = optimalK)
# calculating misclassification rate
sizeTestSet = dim(testingSet)[1]
# get the number of data points that are misclassified
error = sum(predictedLabels != actualRatings$like)
# calculate the misclassification rate
misclassification_rate = error/sizeTestSet
#Display the masclassification rate
print(misclassification_rate)
results = c()
i = 1
while(i <= length(predictedLabels))
{
if(predictedLabels[i] != actualRatings$like[i] && predictedLabels[i] == ":)")
{
results<-append(results, predictedLabels[i])
}
i = i+1
}
falseRecommendations = length(results)
falseRecRate = falseRecommendations/sizeTestSet
library(class)
#importing data sets
labeledData <- read.csv("spotifyMusic_labledSet.csv")
unlabeledData <- read.csv("spotifyMusic_unlabledSet.csv")
actualRatings <- read.csv("true_like.csv")
# allows instructor to re-run code with my results from previous run so the written answers to questions makes sense
set.seed(1000)
# normalizing data parameters of training labeled data
labeledData$acousticness = (labeledData$acousticness - mean(labeledData$acousticness))/sd(labeledData$acousticness)
labeledData$danceability = (labeledData$danceability - mean(labeledData$danceability))/sd(labeledData$danceability)
labeledData$energy = (labeledData$energy - mean(labeledData$energy))/sd(labeledData$energy)
labeledData$instrumentalness = (labeledData$instrumentalness- mean(labeledData$instrumentalness))/sd(labeledData$instrumentalness)
labeledData$liveness = (labeledData$liveness- mean(labeledData$liveness))/sd(labeledData$liveness)
labeledData$loudness = (labeledData$loudness- mean(labeledData$loudness))/sd(labeledData$loudness)
labeledData$speechiness = (labeledData$loudness - mean(labeledData$loudness))/sd(labeledData$loudness)
labeledData$tempo = (labeledData$tempo-mean(labeledData$tempo))/sd(labeledData$tempo)
labeledData$valence = (labeledData$valence - mean(labeledData$valence))/sd(labeledData$valence)
# normalizing data parameters of testing unlabeled data
unlabeledData$acousticness = (unlabeledData$acousticness - mean(unlabeledData$acousticness))/sd(unlabeledData$acousticness)
unlabeledData$danceability = (unlabeledData$danceability - mean(unlabeledData$danceability))/sd(unlabeledData$danceability)
unlabeledData$energy = (unlabeledData$energy - mean(unlabeledData$energy))/sd(unlabeledData$energy)
unlabeledData$instrumentalness = (unlabeledData$instrumentalness- mean(unlabeledData$instrumentalness))/sd(unlabeledData$instrumentalness)
unlabeledData$liveness = (unlabeledData$liveness- mean(unlabeledData$liveness))/sd(unlabeledData$liveness)
unlabeledData$loudness = (unlabeledData$loudness- mean(unlabeledData$loudness))/sd(unlabeledData$loudness)
unlabeledData$speechiness = (unlabeledData$loudness - mean(unlabeledData$loudness))/sd(unlabeledData$loudness)
unlabeledData$tempo = (unlabeledData$tempo-mean(unlabeledData$tempo))/sd(unlabeledData$tempo)
unlabeledData$valence = (unlabeledData$valence - mean(unlabeledData$valence))/sd(unlabeledData$valence)
num_samples = dim(labeledData)[1]
sampling.rate = 0.8
kTestVals = c()
# Running K-fold cross validation for K values from 1-10
for(k in 1:10)
{
AllErrors = c()
for(fold in 1:5)
{
# creating Training and testing Sets
training <- sample(1:num_samples, sampling.rate*num_samples, replace=FALSE)
trainingSet <- subset(labeledData[training, ])
testing <- setdiff(1:num_samples, training)
testingSet <- subset(labeledData[testing,])
# Getting features of training set
trainingfeatures <- subset(trainingSet, select=c(-like))
# Get the labels of the training set
traininglabels <- trainingSet$like
#Get the features of the testing set
testingfeatures <- subset(testingSet, select=c(-like))
predictions = knn(trainingfeatures, testingfeatures, traininglabels, k = k)
errors = sum(predictions != testingSet$like)
mse = mean(errors^2)
AllErrors[fold] = mse
}
AverageError = mean(AllErrors)
kTestVals[k] = AverageError
}
# Retrieving K value with smallest average error after validation test
optimalK = which.min(kTestVals)
# partitioning training and testing sets
trainingSet <- labeledData
testingSet <- unlabeledData
# Getting features of training set
trainingfeatures <- subset(trainingSet, select=c(-like))
# Get the labels of the training set
traininglabels <- trainingSet$like
#Get the features of the testing set
testingfeatures <- subset(testingSet, select=c(-like))
predictions = knn(trainingfeatures, testingfeatures, traininglabels, k = optimalK)
print("Question 1. KNN results:")
print(predictions)
library(class)
predictedLabels = knn(trainingfeatures, testingfeatures, traininglabels, k = optimalK)
# calculating misclassification rate
sizeTestSet = dim(testingSet)[1]
# get the number of data points that are misclassified
error = sum(predictedLabels != actualRatings$like)
# calculate the misclassification rate
misclassification_rate = error/sizeTestSet
#Display the masclassification rate
print(misclassification_rate)
results = c()
i = 1
while(i <= length(predictedLabels))
{
if(predictedLabels[i] != actualRatings$like[i] && predictedLabels[i] == ":)")
{
results<-append(results, predictedLabels[i])
}
i = i+1
}
falseRecommendations = length(results)
falseRecRate = falseRecommendations/sizeTestSet
library(class)
#importing data sets
labeledData <- read.csv("spotifyMusic_labledSet.csv")
unlabeledData <- read.csv("spotifyMusic_unlabledSet.csv")
actualRatings <- read.csv("true_like.csv")
# allows instructor to re-run code with my results from previous run so the written answers to questions makes sense
set.seed(1000)
# normalizing data parameters of training labeled data
labeledData$acousticness = (labeledData$acousticness - mean(labeledData$acousticness))/sd(labeledData$acousticness)
labeledData$danceability = (labeledData$danceability - mean(labeledData$danceability))/sd(labeledData$danceability)
labeledData$energy = (labeledData$energy - mean(labeledData$energy))/sd(labeledData$energy)
labeledData$instrumentalness = (labeledData$instrumentalness- mean(labeledData$instrumentalness))/sd(labeledData$instrumentalness)
labeledData$liveness = (labeledData$liveness- mean(labeledData$liveness))/sd(labeledData$liveness)
labeledData$loudness = (labeledData$loudness- mean(labeledData$loudness))/sd(labeledData$loudness)
labeledData$speechiness = (labeledData$loudness - mean(labeledData$loudness))/sd(labeledData$loudness)
labeledData$tempo = (labeledData$tempo-mean(labeledData$tempo))/sd(labeledData$tempo)
labeledData$valence = (labeledData$valence - mean(labeledData$valence))/sd(labeledData$valence)
# normalizing data parameters of testing unlabeled data
unlabeledData$acousticness = (unlabeledData$acousticness - mean(unlabeledData$acousticness))/sd(unlabeledData$acousticness)
unlabeledData$danceability = (unlabeledData$danceability - mean(unlabeledData$danceability))/sd(unlabeledData$danceability)
unlabeledData$energy = (unlabeledData$energy - mean(unlabeledData$energy))/sd(unlabeledData$energy)
unlabeledData$instrumentalness = (unlabeledData$instrumentalness- mean(unlabeledData$instrumentalness))/sd(unlabeledData$instrumentalness)
unlabeledData$liveness = (unlabeledData$liveness- mean(unlabeledData$liveness))/sd(unlabeledData$liveness)
unlabeledData$loudness = (unlabeledData$loudness- mean(unlabeledData$loudness))/sd(unlabeledData$loudness)
unlabeledData$speechiness = (unlabeledData$loudness - mean(unlabeledData$loudness))/sd(unlabeledData$loudness)
unlabeledData$tempo = (unlabeledData$tempo-mean(unlabeledData$tempo))/sd(unlabeledData$tempo)
unlabeledData$valence = (unlabeledData$valence - mean(unlabeledData$valence))/sd(unlabeledData$valence)
num_samples = dim(labeledData)[1]
sampling.rate = 0.8
kTestVals = c()
# Running K-fold cross validation for K values from 1-10
for(k in 1:10)
{
AllErrors = c()
for(fold in 1:5)
{
# creating Training and testing Sets
training <- sample(1:num_samples, sampling.rate*num_samples, replace=FALSE)
trainingSet <- subset(labeledData[training, ])
testing <- setdiff(1:num_samples, training)
testingSet <- subset(labeledData[testing,])
# Getting features of training set
trainingfeatures <- subset(trainingSet, select=c(-like))
# Get the labels of the training set
traininglabels <- trainingSet$like
#Get the features of the testing set
testingfeatures <- subset(testingSet, select=c(-like))
predictions = knn(trainingfeatures, testingfeatures, traininglabels, k = k)
errors = sum(predictions != testingSet$like)
mse = mean(errors^2)
AllErrors[fold] = mse
}
AverageError = mean(AllErrors)
kTestVals[k] = AverageError
}
# Retrieving K value with smallest average error after validation test
optimalK = which.min(kTestVals)
# partitioning training and testing sets
trainingSet <- labeledData
testingSet <- unlabeledData
# Getting features of training set
trainingfeatures <- subset(trainingSet, select=c(-like))
# Get the labels of the training set
traininglabels <- trainingSet$like
#Get the features of the testing set
testingfeatures <- subset(testingSet, select=c(-like))
predictions = knn(trainingfeatures, testingfeatures, traininglabels, k = optimalK)
print("Question 1. KNN results:")
print(predictions)
library(class)
predictedLabels = knn(trainingfeatures, testingfeatures, traininglabels, k = optimalK)
# calculating misclassification rate
sizeTestSet = dim(testingSet)[1]
# get the number of data points that are misclassified
error = sum(predictedLabels != actualRatings$like)
# calculate the misclassification rate
misclassification_rate = error/sizeTestSet
#Display the masclassification rate
print(error)
print(misclassification_rate)
results = c()
i = 1
while(i <= length(predictedLabels))
{
if(predictedLabels[i] != actualRatings$like[i] && predictedLabels[i] == ":)")
{
results<-append(results, predictedLabels[i])
}
i = i+1
}
falseRecommendations = length(results)
falseRecRate = falseRecommendations/sizeTestSet
results = c()
i = 1
while(i <= length(predictedLabels))
{
if(predictedLabels[i] != actualRatings$like[i] && predictedLabels[i] == ":)")
{
results<-append(results, predictedLabels[i])
}
i = i+1
}
falseRecommendations = length(results)
falseRecRate = falseRecommendations/sizeTestSet
print(falseRecommendations)
print(fakseRecRate)
results = c()
i = 1
while(i <= length(predictedLabels))
{
if(predictedLabels[i] != actualRatings$like[i] && predictedLabels[i] == ":)")
{
results<-append(results, predictedLabels[i])
}
i = i+1
}
falseRecommendations = length(results)
falseRecRate = falseRecommendations/sizeTestSet
print(falseRecommendations)
print(falseRecRate)
library(class)
#importing data sets
labeledData <- read.csv("spotifyMusic_labledSet.csv")
unlabeledData <- read.csv("spotifyMusic_unlabledSet.csv")
actualRatings <- read.csv("true_like.csv")
# allows instructor to re-run code with my results from previous run so the written answers to questions makes sense
set.seed(1000)
# normalizing data parameters of training labeled data
labeledData$acousticness = (labeledData$acousticness - mean(labeledData$acousticness))/sd(labeledData$acousticness)
labeledData$danceability = (labeledData$danceability - mean(labeledData$danceability))/sd(labeledData$danceability)
labeledData$energy = (labeledData$energy - mean(labeledData$energy))/sd(labeledData$energy)
labeledData$instrumentalness = (labeledData$instrumentalness- mean(labeledData$instrumentalness))/sd(labeledData$instrumentalness)
labeledData$liveness = (labeledData$liveness- mean(labeledData$liveness))/sd(labeledData$liveness)
labeledData$loudness = (labeledData$loudness- mean(labeledData$loudness))/sd(labeledData$loudness)
labeledData$speechiness = (labeledData$loudness - mean(labeledData$loudness))/sd(labeledData$loudness)
labeledData$tempo = (labeledData$tempo-mean(labeledData$tempo))/sd(labeledData$tempo)
labeledData$valence = (labeledData$valence - mean(labeledData$valence))/sd(labeledData$valence)
# normalizing data parameters of testing unlabeled data
unlabeledData$acousticness = (unlabeledData$acousticness - mean(unlabeledData$acousticness))/sd(unlabeledData$acousticness)
unlabeledData$danceability = (unlabeledData$danceability - mean(unlabeledData$danceability))/sd(unlabeledData$danceability)
unlabeledData$energy = (unlabeledData$energy - mean(unlabeledData$energy))/sd(unlabeledData$energy)
unlabeledData$instrumentalness = (unlabeledData$instrumentalness- mean(unlabeledData$instrumentalness))/sd(unlabeledData$instrumentalness)
unlabeledData$liveness = (unlabeledData$liveness- mean(unlabeledData$liveness))/sd(unlabeledData$liveness)
unlabeledData$loudness = (unlabeledData$loudness- mean(unlabeledData$loudness))/sd(unlabeledData$loudness)
unlabeledData$speechiness = (unlabeledData$loudness - mean(unlabeledData$loudness))/sd(unlabeledData$loudness)
unlabeledData$tempo = (unlabeledData$tempo-mean(unlabeledData$tempo))/sd(unlabeledData$tempo)
unlabeledData$valence = (unlabeledData$valence - mean(unlabeledData$valence))/sd(unlabeledData$valence)
num_samples = dim(labeledData)[1]
sampling.rate = 0.8
kTestVals = c()
# Running K-fold cross validation for K values from 1-10
for(k in 1:10)
{
AllErrors = c()
for(fold in 1:5)
{
# creating Training and testing Sets
training <- sample(1:num_samples, sampling.rate*num_samples, replace=FALSE)
trainingSet <- subset(labeledData[training, ])
testing <- setdiff(1:num_samples, training)
testingSet <- subset(labeledData[testing,])
# Getting features of training set
trainingfeatures <- subset(trainingSet, select=c(-like))
# Get the labels of the training set
traininglabels <- trainingSet$like
#Get the features of the testing set
testingfeatures <- subset(testingSet, select=c(-like))
predictions = knn(trainingfeatures, testingfeatures, traininglabels, k = k)
errors = sum(predictions != testingSet$like)
mse = mean(errors^2)
AllErrors[fold] = mse
}
AverageError = mean(AllErrors)
kTestVals[k] = AverageError
}
# Retrieving K value with smallest average error after validation test
optimalK = which.min(kTestVals)
# partitioning training and testing sets
trainingSet <- labeledData
testingSet <- unlabeledData
# Getting features of training set
trainingfeatures <- subset(trainingSet, select=c(-like))
# Get the labels of the training set
traininglabels <- trainingSet$like
#Get the features of the testing set
testingfeatures <- subset(testingSet, select=c(-like))
predictions = knn(trainingfeatures, testingfeatures, traininglabels, k = optimalK)
print("Question 1. KNN results:")
print(predictions)
library(class)
#importing data sets
labeledData <- read.csv("spotifyMusic_labledSet.csv")
unlabeledData <- read.csv("spotifyMusic_unlabledSet.csv")
actualRatings <- read.csv("true_like.csv")
# allows instructor to re-run code with my results from previous run so the written answers to questions makes sense
set.seed(1000)
# normalizing data parameters of training labeled data
labeledData$acousticness = (labeledData$acousticness - mean(labeledData$acousticness))/sd(labeledData$acousticness)
labeledData$danceability = (labeledData$danceability - mean(labeledData$danceability))/sd(labeledData$danceability)
labeledData$energy = (labeledData$energy - mean(labeledData$energy))/sd(labeledData$energy)
labeledData$instrumentalness = (labeledData$instrumentalness- mean(labeledData$instrumentalness))/sd(labeledData$instrumentalness)
labeledData$liveness = (labeledData$liveness- mean(labeledData$liveness))/sd(labeledData$liveness)
labeledData$loudness = (labeledData$loudness- mean(labeledData$loudness))/sd(labeledData$loudness)
labeledData$speechiness = (labeledData$loudness - mean(labeledData$loudness))/sd(labeledData$loudness)
labeledData$tempo = (labeledData$tempo-mean(labeledData$tempo))/sd(labeledData$tempo)
labeledData$valence = (labeledData$valence - mean(labeledData$valence))/sd(labeledData$valence)
# normalizing data parameters of testing unlabeled data
unlabeledData$acousticness = (unlabeledData$acousticness - mean(unlabeledData$acousticness))/sd(unlabeledData$acousticness)
unlabeledData$danceability = (unlabeledData$danceability - mean(unlabeledData$danceability))/sd(unlabeledData$danceability)
unlabeledData$energy = (unlabeledData$energy - mean(unlabeledData$energy))/sd(unlabeledData$energy)
unlabeledData$instrumentalness = (unlabeledData$instrumentalness- mean(unlabeledData$instrumentalness))/sd(unlabeledData$instrumentalness)
unlabeledData$liveness = (unlabeledData$liveness- mean(unlabeledData$liveness))/sd(unlabeledData$liveness)
unlabeledData$loudness = (unlabeledData$loudness- mean(unlabeledData$loudness))/sd(unlabeledData$loudness)
unlabeledData$speechiness = (unlabeledData$loudness - mean(unlabeledData$loudness))/sd(unlabeledData$loudness)
unlabeledData$tempo = (unlabeledData$tempo-mean(unlabeledData$tempo))/sd(unlabeledData$tempo)
unlabeledData$valence = (unlabeledData$valence - mean(unlabeledData$valence))/sd(unlabeledData$valence)
num_samples = dim(labeledData)[1]
sampling.rate = 0.8
kTestVals = c()
# Running K-fold cross validation for K values from 1-10
for(k in 1:10)
{
AllErrors = c()
for(fold in 1:5)
{
# creating Training and testing Sets
training <- sample(1:num_samples, sampling.rate*num_samples, replace=FALSE)
trainingSet <- subset(labeledData[training, ])
testing <- setdiff(1:num_samples, training)
testingSet <- subset(labeledData[testing,])
# Getting features of training set
trainingfeatures <- subset(trainingSet, select=c(-like))
# Get the labels of the training set
traininglabels <- trainingSet$like
#Get the features of the testing set
testingfeatures <- subset(testingSet, select=c(-like))
predictions = knn(trainingfeatures, testingfeatures, traininglabels, k = k)
errors = sum(predictions != testingSet$like)
mse = mean(errors^2)
AllErrors[fold] = mse
}
AverageError = mean(AllErrors)
kTestVals[k] = AverageError
}
# Retrieving K value with smallest average error after validation test
optimalK = which.min(kTestVals)
# partitioning training and testing sets
trainingSet <- labeledData
testingSet <- unlabeledData
# Getting features of training set
trainingfeatures <- subset(trainingSet, select=c(-like))
# Get the labels of the training set
traininglabels <- trainingSet$like
#Get the features of the testing set
testingfeatures <- subset(testingSet, select=c(-like))
predictions = knn(trainingfeatures, testingfeatures, traininglabels, k = optimalK)
print("Question 1. KNN results:")
print(predictions)
library(class)
predictedLabels = knn(trainingfeatures, testingfeatures, traininglabels, k = optimalK)
# calculating misclassification rate
sizeTestSet = dim(testingSet)[1]
# get the number of data points that are misclassified
error = sum(predictedLabels != actualRatings$like)
# calculate the misclassification rate
misclassification_rate = error/sizeTestSet
#Display the masclassification rate
print(error)
print(misclassification_rate)
results = c()
i = 1
while(i <= length(predictedLabels))
{
if(predictedLabels[i] != actualRatings$like[i] && predictedLabels[i] == ":)")
{
results<-append(results, predictedLabels[i])
}
i = i+1
}
falseRecommendations = length(results)
falseRecRate = falseRecommendations/sizeTestSet
print(falseRecommendations)
print(falseRecRate)
setwd("~/Desktop/Data-Sci-Final-Project")
mvpData <- read.csv("master_table.csv")
View(mvpData)
mvpData <- read.csv("master_table.csv")
round(cor(mvpData[,1:5]),2)
mvpData <- read.csv("master_table.csv")
round(cor(mvpData[,5:100]),2)
mvpData <- read.csv("master_table.csv")
round(cor(mvpData[,5:45]),2)
mvpData <- read.csv("master_table.csv")
round(cor(mvpData[,5:40]),2)
mvpData <- read.csv("master_table.csv")
numericData <- mvpData
numericData$team = NULL
View(numericData)
mvpData <- read.csv("master_table.csv")
numericData <- mvpData
numericData$team = NULL
numericData$Rank = NULL
numericData$Player = NULL
numericData$Tm = NULL
mvpData <- read.csv("master_table.csv")
numericData <- mvpData
numericData$team = NULL
numericData$Rank = NULL
numericData$Player = NULL
numericData$Tm = NULL
round(cor(numericData))
mvpData <- read.csv("master_table.csv")
numericData <- mvpData
numericData$team = NULL
numericData$Rank = NULL
numericData$Player = NULL
numericData$Tm = NULL
round(cor(numericData), 2)
mvpData <- read.csv("master_table.csv")
numericData <- mvpData
numericData$team = NULL
numericData$Rank = NULL
numericData$Player = NULL
numericData$Tm = NULL
correlation <- round(cor(numericData), 2)
View(correlation)
mvpData <- read.csv("master_table.csv")
numericData <- mvpData
numericData$team = NULL
numericData$Rank = NULL
numericData$Player = NULL
numericData$Tm = NULL
correlation <- round(cor(numericData), 2)
print(correlation$Share)
mvpData <- read.csv("master_table.csv")
numericData <- mvpData
numericData$team = NULL
numericData$Rank = NULL
numericData$Player = NULL
numericData$Tm = NULL
correlation <- round(cor(numericData), 2)
mvpData <- read.csv("master_table.csv")
numericData <- mvpData
numericData$team = NULL
numericData$Rank = NULL
numericData$Player = NULL
numericData$Tm = NULL
correlation <- round(cor(numericData), 2)
results = lm(numericData$Share ~ ., data = numericData)
mvpData <- read.csv("master_table.csv")
numericData <- mvpData
numericData$team = NULL
numericData$Rank = NULL
numericData$Player = NULL
numericData$Tm = NULL
correlation <- round(cor(numericData), 2)
results = lm(numericData$Share ~ ., data = numericData)
summary(results)
